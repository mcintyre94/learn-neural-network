{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # Allow printing multiple outputs from each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Dependency for the computations\n",
    "import numpy as np\n",
    "\n",
    "# Dependency only for the charts\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # Make matplotlib charts look better\n",
    "sns.set(style='ticks')\n",
    "\n",
    "# Dependency only for adding methods to classes dynamically in jupyter (Jupyter Dynamic Classes)\n",
    "# See https://github.com/jupyter/notebook/issues/1243 and https://alexhagen.github.io/jdc/\n",
    "import jdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A literate programming notebook written as I follow http://neuralnetworksanddeeplearning.com/chap1.html (as recommended in fast.ai course lesson 2 notes)\n",
    "\n",
    "This is basically a 'My First Neural Network' notebook with (hopefully) copious documentation, built to learn NumPy and solidify the concepts for me before going back to using the higher-level libraries such as Keras/TensorFlow. It won't use GPU, and it isn't optimised for performance. It's just about checking I understand the concepts fully, and to explore the NumPy APIs.\n",
    "\n",
    "The end goal (based on the chapter 1 content) is to build an entry for the Kaggle MNIST competition, using a neural network, written from scratch - and for that neural network to be general enough for other problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons\n",
    "\n",
    "A perceptron takes any number of binary inputs $x_0, x_1,..., x_n$ and outputs a single binary value. First we compute $\\sum_j x_j w_j$, where each $w_j \\in \\mathbb{R}$ is a weight (and there are equal numbers of inputs $x_i$ and weights $w_i$). This is the dot product of the two vectors, $\\mathbf{x} \\cdot \\mathbf{w}$, which we can compute using the `np.dot` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "x = [0, 0, 1, 1, 0] # 5 binary inputs\n",
    "w = [1, 2, 3, 4, 1] # 5 real-valued weights\n",
    "\n",
    "dot = np.dot(x, w)\n",
    "print(dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for a perceptron we see that because of the binary-valued inputs, the output of $\\mathbf{x} \\cdot \\mathbf{w}$ is simply the sum of weights $w_j$ where $x_j = 1$. \n",
    "\n",
    "As mentioned, the output of a perceptron is in fact binary-valued, and this is done using a threshold: If $\\mathbf{x} \\cdot \\mathbf{w} \\le threshold$ then the perceptron outputs 0, otherwise it outputs 1. \n",
    "\n",
    "We replace the threshold with bias $-b = threshold$, and move it to the other side of the inequality, such that we have: If $\\mathbf{x} \\cdot \\mathbf{w} + b \\le 0$ then the perceptron outputs 0, otherwise it outputs 1. \n",
    "\n",
    "So we could fully capture a perceptron's behaviour with a simple method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perceptron(x, w, bias):\n",
    "    if np.dot(x, w) + bias <= 0:\n",
    "        return 0\n",
    "    else: \n",
    "        return 1\n",
    "\n",
    "# For example, using our x and w from before\n",
    "x = [0, 0, 1, 1, 0] \n",
    "w = [1, 2, 3, 4, 1] \n",
    "perceptron(x, w, bias=2)\n",
    "perceptron(x, w, bias=-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From the book, using the perceptron to implement NAND \n",
    "w = [-2, -2]\n",
    "bias = 3\n",
    "\n",
    "for x1, x2 in [(0,0), (0,1), (1,0), (1,1)]:\n",
    "    perceptron([x1,x2], w, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can in fact use a perceptron to learn any linearly separable function, including NAND. To show this, let's consider how a perceptron learns. \n",
    "\n",
    "We start with randomly initialised weights and bias terms. We also pass in training data, in tuples of (input, output) ie (x, y). For a binary function this is ((x1, x2), y).\n",
    "\n",
    "Then for each **epoch** (ie the number of times we want to learn), we take each training example, and using its input and our current weights we compute the current value of the perceptron on that input. We find the difference between the expected output `y` and the current output, the error term. Note that this must be computed `y - output` - the order matters. The error term will either be 0, 1 or -1. \n",
    "\n",
    "We adjust the weights in each position $w_i$ by $w_i += \\eta * error * x_i$ ($\\eta$, eta is just a learning rate that we can tweak - it changes how quickly the weights change for wrong examples.) We adjust the bias by $\\eta * error$, since it's not linked to any input directly. \n",
    "\n",
    "Assuming the input training data is linearly separable this algorithm will converge to 100% accuracy in a finite number of epochs. \n",
    "\n",
    "\n",
    "To demonstrate this, we'll define a function to train a perceptron - and demonstrate it on the NAND case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(training_data, epochs=30, eta=1):\n",
    "    x_len = len(training_data[0][0]) # Tuples of ((xs...), y) - want len xs\n",
    "\n",
    "    w = np.random.rand(x_len) # Random starting weights equal length to x_len, 0-1\n",
    "    bias = np.random.rand() # Random starting bias, 0-1\n",
    "\n",
    "    errors = {} # Count errors at each epoch\n",
    "\n",
    "    for i in range(epochs):\n",
    "        errors[i] = 0\n",
    "        for x, y in training_data:\n",
    "            output = perceptron(x, w, bias)\n",
    "            error = y - output\n",
    "            if error != 0:\n",
    "                errors[i]+=1\n",
    "\n",
    "            w[0] += eta * error * x[0]\n",
    "            w[1] += eta * error * x[1]\n",
    "            bias += eta * error\n",
    "    \n",
    "    return w, bias, errors\n",
    "\n",
    "\n",
    "training_data = [((0,0), 1), ((0, 1), 1), ((1,0), 1), ((1,1), 0)] # Tuples of ((x1,x2), x1 NAND x2)\n",
    "\n",
    "w, b, errors = train_perceptron(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10640ab90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,u'epoch')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,u'errors')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,u'Errors at each epoch')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10640acd0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEZCAYAAACEkhK6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGCIiUhIFhoiIlOT/AzaFGVXPgOG1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10640ab90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "xs = errors.keys()\n",
    "ys = errors.values()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('errors')\n",
    "plt.title('Errors at each epoch')\n",
    "plt.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(-1, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,u'Separation boundary found')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x106c312d0>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAELCAYAAAAlTtoUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106a2ef90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_intersect = [0,-bias/w[1]]\n",
    "y_intersect = [-bias/w[0],0]\n",
    "plt.xlim([-1,1])\n",
    "plt.ylim([-1,1])\n",
    "plt.title('Separation boundary found')\n",
    "\n",
    "plt.plot(x_intersect, y_intersect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the effect of using a smaller eta variable on the same training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Neuron instance has no __call__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-263599a8ed08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_perceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-83037dcc27ce>\u001b[0m in \u001b[0;36mtrain_perceptron\u001b[0;34m(training_data, epochs, eta)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Neuron instance has no __call__ method"
     ]
    }
   ],
   "source": [
    "w, b, errors = train_perceptron(training_data, eta=0.05)\n",
    "\n",
    "plt.figure()\n",
    "xs = errors.keys()\n",
    "ys = errors.values()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('errors')\n",
    "plt.title('Errors at each epoch')\n",
    "plt.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons have some limitations:\n",
    "\n",
    "- They only converge for linearly separable problems, which most interesting problems are not\n",
    "- Because they use a step function, we can't make small tweaks to their output - it's either 0 or 1\n",
    "\n",
    "The way we deal with the first issue will be by building networks of neurons, where the output of one layer can be inputs to the next layer. This allows us to solve complex problems (or, mathematically - can approximate to an arbitrary accuracy any function). However this exasperates our second problem - without being able to tweak our perceptrons in a predictable way we're not going to be able to create a trainable network of them. \n",
    "\n",
    "For this reason, we change the non-linearity, that is the **activation function**. Our computed $\\mathbf{x} \\cdot \\mathbf{w} + b$ doesn't need to change, but instead of using the inequality with 0, we pass this value to a different function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid neurons\n",
    "\n",
    "The sigmoid neuron uses the activation function $\\sigma(z) = \\frac{1}{1+e^-z}$, where z is our computed $\\mathbf{x} \\cdot \\mathbf{w} + b$. Before we code this, we can examine its shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_fn(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,u'Sigmoid')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1066cef50>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAELCAYAAADeNe2OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1066e1b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = np.linspace(-5,5,100) # 100 evenly spaced values between -5 and 5\n",
    "ys = sigmoid_fn(xs)\n",
    "\n",
    "plt.title('Sigmoid')\n",
    "plt.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,u'Step function')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10672a890>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAELCAYAAADeNe2OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10640aa10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare this to the step function:\n",
    "\n",
    "def step(x):\n",
    "    return 1 if x>0 else 0\n",
    "\n",
    "xs = np.linspace(-5,5,100) # 100 evenly spaced values between -5 and 5\n",
    "ys = [step(x) for x in xs]\n",
    "\n",
    "plt.title('Step function')\n",
    "plt.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the sigmoid is a smoothed step function, maintaining the bounds of 0 and 1, but changing by only a small amount each time its input changes. This is the only difference between the sigmoid and perceptron, but it turns out to be significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it makes sense to generalise our code to define a neuron, with a view to eventually building up a neural network - that is, a network of layers of neurons. So let's define a neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, num_inputs, activation_function):\n",
    "        # We always initialise a neuron with random weights and bias\n",
    "        # We must pass an activation function, as this is different for each type of neuron\n",
    "        self.weights = np.random.rand(num_inputs)\n",
    "        self.bias = np.random.rand()\n",
    "        self.num_inputs = num_inputs # This will let us generalise updates\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "    def _compute_value(self, xs):\n",
    "        # Compute the value of the neuron for the given inputs\n",
    "        return np.dot(xs, self.weights) + self.bias\n",
    "    \n",
    "    def get_value(self, xs):\n",
    "        return self.activation_function(self._compute_value(xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, a neuron knows how to compute its value from its weights and bias term, and it has an activation function, which takes such a value and computes the desired output for the neuron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.63452654848784429"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So a perceptron is simply:\n",
    "perceptron = Neuron(2, activation_function = step)\n",
    "perceptron.get_value([0,0])\n",
    "\n",
    "# And a sigmoid is simply:\n",
    "sigmoid = Neuron(2, activation_function = sigmoid_fn)\n",
    "sigmoid.get_value([0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can re-implement our learning method using the generic neuron (and thus supporting any given activation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Neuron\n",
    "def train(self, training_data, epochs=30, eta=0.05):\n",
    "    errors = {}\n",
    "\n",
    "    for i in range(epochs):\n",
    "        errors[i] = 0\n",
    "        for X, y in training_data:\n",
    "            output = self.get_value(X)\n",
    "            error = y - output\n",
    "            if np.absolute(error) > 0.5: # Not a binary output, so allow some difference here - as long as we're close\n",
    "                errors[i]+=1\n",
    "\n",
    "            # Update each weight in turn, and the bias\n",
    "            for j in range(self.num_inputs):\n",
    "                self.weights[j] += eta * error * X[j]\n",
    "            self.bias += eta * error\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = Neuron(2, activation_function = step) # Make sure we re-create the neuron to get the new method included\n",
    "errors = sigmoid.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(-1, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,u'Separation boundary found')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10640a950>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAELCAYAAAAlTtoUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10640aad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_intersect = [0,-sigmoid.bias/sigmoid.weights[1]]\n",
    "y_intersect = [-sigmoid.bias/sigmoid.weights[0],0]\n",
    "plt.xlim([-1,1])\n",
    "plt.ylim([-1,1])\n",
    "plt.title('Separation boundary found')\n",
    "plt.plot(x_intersect, y_intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10697ee50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,u'epoch')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,u'errors')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,u'Errors at each epoch')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10697edd0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEZCAYAAABWwhjiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiIlEwhIiIiJVOIiIhIyRQiIiJSsv8PsL4Ab+IF7gEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10697ee50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "xs = errors.keys()\n",
    "ys = errors.values()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('errors')\n",
    "plt.title('Errors at each epoch')\n",
    "plt.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A layer of neurons: Naive\n",
    "\n",
    "Like a perceptron, a sigmoid can only classify linearly separable problems. To solve general problems we need to have multiple layers of neurons - and because each neuron has a non-linear activation function this will provide us a non-linear system and allow us to approximate non-linearly separable (actually, all) functions. \n",
    "\n",
    "Let's first consider what a layer of neurons actually looks like, in the simplest way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:     \n",
    "    def __init__(self):\n",
    "        self.neurons = []\n",
    "    \n",
    "    def add_neuron(self, neuron):\n",
    "        self.neurons.append(neuron)\n",
    "        \n",
    "    def evaluate(self, xs):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.get_value(xs))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A layer of neurons just contains the neurons that have been added to it. It receives some input - just like a single neuron does - but its output contains the computed value of each neuron in the layer.\n",
    "\n",
    "The way to think of this is as the layer receiving the output of the previous layer, or the inputs to the network as a whole. Then the outputs of this layer could be the outputs of the network as a whole, or they could be fed forward into the next layer. \n",
    "\n",
    "In each layer, every neuron receives as input the complete output of the last layer. We call these layers 'fully connected' or 'dense', and they're the only sort of layer we'll consider here. \n",
    "\n",
    "Consider a layer of 3 neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.049177462756930544, 0.02888933593984274, 0.0019240636538269203]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[0.57437706909313158,\n",
       " 0.73869048625923339,\n",
       " 0.69071199929040938,\n",
       " 0.87125647208647572,\n",
       " 0.73249934220212476,\n",
       " 0.59309161142966804]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = Layer()\n",
    "for i in range(3): # Add 3 neurons\n",
    "    sig = Neuron(4, activation_function = sigmoid_fn) # Each sigmoid takes 4 inputs\n",
    "    layer.add_neuron(sig)\n",
    "\n",
    "layer.evaluate([1,-4,3,-6]) # We can evaluate any 4 real numbers as input\n",
    "\n",
    "for i in range(3): # Add 3 more neurons\n",
    "    sig = Neuron(4, activation_function = sigmoid_fn) # Each sigmoid takes 4 inputs\n",
    "    layer.add_neuron(sig)\n",
    "\n",
    "layer.evaluate([1, 0, 1, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our layer takes as input 4 values, and each of its neurons can take as input such a list. It evaluates to 3 outputs, because we added 3 neurons. When we add another 3 neurons for a total of 6, it evaluates to 6 outputs.\n",
    "\n",
    "Clearly all of our neurons need to share the same number of inputs - the same input it passed to all of them when we evaluate them. A better API would enforce this constraint. I'm not sure if there's ever a reason to mix-and-match activation functions within a layer, but we're not going to do that here - so let's also make the activation function a parameter of the `Layer` and enforce that it's the same for all neurons. Here's a better API for a `Layer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.99436681343947719, 0.9670452402866816, 0.98368409106320842]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[0.39503501971197053,\n",
       " 0.78934554680822933,\n",
       " 0.50501476120121269,\n",
       " 0.68461699203082227,\n",
       " 0.48765506982745527,\n",
       " 0.77877715258694657]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer:\n",
    "    def __init__(self, num_inputs, num_neurons, activation_function):\n",
    "        self.neurons = []\n",
    "        for i in range(num_neurons):\n",
    "            neuron = Neuron(num_inputs, activation_function)\n",
    "            self.neurons.append(neuron)\n",
    "            \n",
    "    def evaluate(self, xs):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.get_value(xs))\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "layer = Layer(4, 3, sigmoid_fn)\n",
    "layer.evaluate([1,2,3,4])\n",
    "\n",
    "layer = Layer(4, 6, sigmoid_fn)\n",
    "layer.evaluate([1,-1,1,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we've lost some power here - we can no longer add to a layer after it's been created. It'd be easy enough to add that back, but we won't need to do that here so we'll keep the simpler class going forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing a layer as a matrix\n",
    "\n",
    "Recall what's happening when we compute the value of each neuron - we compute $\\mathbf{w} \\cdot \\mathbf{x} + b$, and then we pass that to the activation function (eg. step or sigmoid).\n",
    "\n",
    "If we say that $\\mathbf{x}$, our input vector, is a column vector (which makes sense graphically if we vertically align neurons within a layer) of length $n$, then we know that $\\mathbf{w}$ is also a vector of length $n$. If we say that $\\mathbf{w}$ is a row vector (or we transpose it from a column vector so that it is), then we can compute the dot product by doing a matrix multiply of the two vectors.\n",
    "\n",
    "For example, let's say that $\\mathbf{w} = \\begin{pmatrix}1 & 2 & 3\\end{pmatrix}$ and $\\mathbf{x} = \\begin{pmatrix}5 \\\\ 10 \\\\ 2 \\end{pmatrix}$, then we can compute their dot product $\\mathbf{w} \\cdot \\mathbf{x} = \\begin{pmatrix}1 & 2 & 3 \\end{pmatrix} \\cdot \\begin{pmatrix}5 \\\\ 10 \\\\ 2\\end{pmatrix}  = (1x5) + (2x10) + (3x2) = 31$\n",
    "\n",
    "If we have a second neuron in the same layer (so receiving the same input $\\mathbf{x}$), with weights $\\mathbf{w'} = \\begin{pmatrix}2 & -2 & 0\\end{pmatrix}$, then we can say that $\\mathbf{w'} \\cdot \\mathbf{x} = \\begin{pmatrix}2 & -2 & 0\\end{pmatrix} \\cdot \\begin{pmatrix}5 \\\\ 10 \\\\ 2 \\end{pmatrix} = (2x5) + (-2x10) + (0x2) = -10$\n",
    "\n",
    "Then we can say that the dot products for our layer are $\\begin{pmatrix}31 \\\\ -10\\end{pmatrix}$. Recall that the value (before the activation function) is $\\mathbf{w} \\cdot \\mathbf{x} + b$ - so we need to deal with our bias terms. Suppose the first neuron had bias $b = 3$ and the second neuron $b' = 5$, then we can add the bias terms using vector addition: $\\begin{pmatrix}31 \\\\ -10\\end{pmatrix} + \\begin{pmatrix}3 \\\\ 5\\end{pmatrix} = \\begin{pmatrix}34 \\\\ -5\\end{pmatrix}$\n",
    "\n",
    "And finally we apply the activation function, say for simplicity that these are perceptrons: $\\begin{pmatrix} 1 \\\\\n",
    "0\\end{pmatrix}$\n",
    "\n",
    "This works, but we haven't actually gained anything - we've just split adding our bias and passing through the activation function to the end, doing it after we've done each dot product - but we're still doing individual dot products, and then combining them into a vector we can add the bias terms to and then element-wise applying the activation function. It turns out that we can use matrix multiplication to compute all the neurons in the layer at the same time. We say that $\\mathbf{W}$ is a matrix, where each row is the weights of a single neuron. So for the above 2-neuron layer:\n",
    "$\\mathbf{W} = \\begin{pmatrix} 1 & 2 & 3 \\\\ 2 & -2 & 0 \\end{pmatrix}$. Now, we use matrix multiplication: \n",
    "$\\begin{pmatrix} 1 & 2 & 3 \\\\ 2 & -2 & 0 \\end{pmatrix} \\cdot \\begin{pmatrix}5 \\\\ 10 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix}31 \\\\ -10 \\end{pmatrix}$\n",
    "\n",
    "We then add our bias terms, and apply the activation function to each element as before. \n",
    "\n",
    "This gives us a different way to view a layer of our network, it's simply a matrix $\\mathbf{W}$ and a vector $\\mathbf{b}$ - we don't need to loop over all our neurons or even define them as their own objects. The matrix has one row for each neuron (representing its weights). The length of each neuron's weights (ie the number of columns) is equal to the input. If we say we have m inputs to our layer, and n neurons in the layer, then our matrix multiply $\\mathbf{W} \\cdot \\mathbf{x}$ looks like $(nxm) \\cdot (mx1) = (nx1)$. This is what we'd expect, and explains why our weight matrix must be that specific shape. Each layer takes as input a column vector, and outputs a column vector of a length determined by its number of neurons.\n",
    "\n",
    "Of course, our last layer needs to have the expected output shape for the problem - and therefore has a known number of rows. We can control the number of neurons, and thus rows, in all other layers though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46587637],\n",
       "       [ 0.30082395]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rewriting our Layer using the matrix multiply form\n",
    "class Layer:\n",
    "        def __init__(self, num_inputs, num_neurons, activation_function):\n",
    "            # We use Xavier initialization, variance = 2/(in size + out size) - this scales our random weights sensibly\n",
    "            # The shape of our weights is n x m, where m = num columns = num_inputs, and n=num rows = num_neurons\n",
    "            variance = 2/float(num_inputs + num_neurons)\n",
    "            self.weights = np.random.normal(scale=variance, size=(num_neurons, num_inputs))           \n",
    "            \n",
    "            # The shape of bias is 1 row, with a column for each neuron - a row vector.\n",
    "            self.bias = np.random.random_sample(size=(num_neurons, 1))\n",
    "            \n",
    "            self.activation_function = activation_function\n",
    "            \n",
    "        def evaluate(self, x):\n",
    "            # First, matrix multiply `W` by `x`, ie. weights by activations\n",
    "            wx = np.matmul(self.weights, x)\n",
    "            \n",
    "            # Add the bias term\n",
    "            add_b = wx + self.bias\n",
    "            \n",
    "            # Pass through the activation function\n",
    "            return self.activation_function(add_b)\n",
    "\n",
    "layer = Layer(3, 2, sigmoid_fn)\n",
    "x = np.array([1,2,3]).reshape(3,1)\n",
    "\n",
    "layer.evaluate(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From a layer to a network\n",
    "\n",
    "Now we have everything that we need to define a simple feed-forward neural network consisting of dense layers of neurons: we'll include our input and output layers as a layer of neurons, and we can have any layers between them - we just need to make sure our dimensions all match up.\n",
    "\n",
    "We won't consider our MNIST dataset in any detail yet, other than to say that it contains images of 28x28 greyscale pixels. One obvious way to represent this as an input would be as 28x28=784 values, with each representing the intensity of one pixel. Our job is to classify which number the image is a drawing of, a single digit 0-9. So our final layer will have 10 outputs representing the digits 0-9, each representing the probability that the image is that digit. We would predict that the drawing is the digit with the highest probability. We call all other layers (those that aren't the input or output layer) hidden layers, and we can choose how many of these we want, and how complex they should be. \n",
    "\n",
    "For example, if we wanted a single hidden layer with 40 neurons, then we'd need two layers of neurons:\n",
    "- A layer that takes as input 784 values, and has 40 neurons, and thus outputs 40 values - our hidden layer\n",
    "- A layer that takes as input 40 values, and has 10 neurons, and thus outputs 10 values - our output layer\n",
    "\n",
    "We'd generate predictions by evaluating the hidden layer with the input of 784 values, and then evaluating the output layer with the input of 40 values resulting from that. \n",
    "\n",
    "If we wanted to have two hidden layers with 50 and 110 neurons respectively, then we'd need three layers of neurons:\n",
    "- A layer that takes as input 784 values, and has 50 neurons, and thus outputs 50 values - our first hidden layer\n",
    "- A layer that takes as input 50 values, and has 110 neurons, and thus outputs 110 values - our second hidden layer\n",
    "- A layer that takes as input 110 values, and has 10 neurons, and thus outputs 10 values - our output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how a network looks in practice\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def evaluate(self, x):\n",
    "        value = x\n",
    "        for layer in self.layers:\n",
    "            value = layer.evaluate(value)\n",
    "        \n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks very similar to our first attempt at a `Layer`, but this time we won't simplify any further: we want to permit any layer to be used in our network (so a network can contain layers with different activation functions), and additionally as each layer has a non-linearity (the activation function) we can't simplify the computation any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our examples of networks from above, using sigmoids:\n",
    "net1 = Network()\n",
    "net1.add_layer(Layer(784, 40, sigmoid_fn)) # Hidden layer\n",
    "net1.add_layer(Layer(40, 10, sigmoid_fn)) # Output layer\n",
    "\n",
    "net2 = Network()\n",
    "net2.add_layer(Layer(784, 50, sigmoid_fn)) # First hidden layer\n",
    "net2.add_layer(Layer(50, 110, sigmoid_fn)) # Second hidden layer\n",
    "net2.add_layer(Layer(110, 10, sigmoid_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.64203966],\n",
       "       [ 0.77246824],\n",
       "       [ 0.67130969],\n",
       "       [ 0.72062288],\n",
       "       [ 0.60142098],\n",
       "       [ 0.55008048],\n",
       "       [ 0.48259137],\n",
       "       [ 0.60766909],\n",
       "       [ 0.65611917],\n",
       "       [ 0.63797544]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.67020011],\n",
       "       [ 0.66508444],\n",
       "       [ 0.50120856],\n",
       "       [ 0.71071854],\n",
       "       [ 0.61189746],\n",
       "       [ 0.59950831],\n",
       "       [ 0.63401026],\n",
       "       [ 0.5323064 ],\n",
       "       [ 0.66570045],\n",
       "       [ 0.66381289]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.random_sample(size=(784,1))\n",
    "\n",
    "net1.evaluate(x)\n",
    "net2.evaluate(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network\n",
    "\n",
    "At this point we've got everything that we need to build a network, made of layers of neurons - to compute any classification problem. We do this by feeding the input through the layers, so we call this a feed-forward network. The problem we have now is training it: we can be given inputs and known outputs, we can pass it the input and observe the output, but we don't yet have a way to adjust the weights. We can't update them like we did for a single neuron - that doesn't capture our more subtle relationship between neurons within our network. \n",
    "\n",
    "This is where backpropagation comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our activation function needs to be differentiable\n",
    "class Sigmoid_cls:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def compute(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def derivative(self, z):\n",
    "        return self.compute(z)*(1-self.compute(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We introduce a cost function, also differentiable (quadratic cost function)\n",
    "class QuadraticCost_cls:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def compute(self, computed, desired):\n",
    "        # Quadratic cost of a single training example\n",
    "        return ((computed - desired) ** 2).mean(axis=None)\n",
    "    \n",
    "    def derivative(self, computed, desired):\n",
    "        # Derivative of the compute function, returned as an array\n",
    "        return (computed - desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Layer\n",
    "# Backprop is simpler if we calculate both the weighted input z(x) ie wx + b, as well as f(z(x)) ie sigmoid\n",
    "# So modify layer:\n",
    "def weighted_input(self, x):\n",
    "    # First, matrix multiply `W` by `x`, ie. weights by activation\n",
    "    wx= np.matmul(self.weights, x)\n",
    "            \n",
    "    # Add the bias term\n",
    "    add_b = wx + self.bias\n",
    "    \n",
    "    # We return without applying the activation function\n",
    "    return add_b\n",
    "\n",
    "def evaluate(self, xs):\n",
    "    # Pass the weighted input through the activation function\n",
    "    return self.activation_function.compute(self.weighted_input(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "# Add the cost function as a value of the network\n",
    "def __init__(self, cost_function):\n",
    "    self.layers = []\n",
    "    self.cost_function = cost_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- We split the forward pass to return z(x), and then pass on f(z(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def backprop(self, network_input, desired_output):\n",
    "    gradient_b = [np.zeros(layer.bias.shape) for layer in self.layers] # layer-by-layer gradients wrt biases\n",
    "    gradient_w = [np.zeros(layer.weights.shape) for layer in self.layers] # layer-by-layer gradients wrt weights\n",
    "    \n",
    "    # Feedforward - pass the input through the current network, same effect as evaluate\n",
    "    activation = network_input # Current activation passing through\n",
    "    activations = [network_input] # Activations, per layer, starting from input\n",
    "    weighted_inputs = [] # Weighted inputs, per layer, starting from first non-input layer. Stored for backward pass.\n",
    "    for layer in self.layers:\n",
    "        z = layer.weighted_input(activation)\n",
    "        weighted_inputs.append(z)\n",
    "        activation = layer.activation_function.compute(z)\n",
    "        activations.append(activation)\n",
    "    \n",
    "    # Backward pass - propagate errors backward\n",
    "    \n",
    "    # First we deal with the last layer\n",
    "    # Last layer is special because:\n",
    "    # 1) It can compare its actual output to the desired output (the others all feed through more layers first)\n",
    "    # 2) It doesn't have any layers after it to feed into\n",
    "    last_layer = self.layers[-1]\n",
    "    last_weighted_input = weighted_inputs[-1]\n",
    "    network_output = activations[-1]   \n",
    "    previous_activations = activations[-2]\n",
    "    \n",
    "    activation_derivative = last_layer.activation_function.derivative(last_weighted_input)\n",
    "    cost_derivative = self.cost_function.derivative(network_output, desired_output)\n",
    "    delta = activation_derivative * cost_derivative\n",
    "    \n",
    "    # Delta is the shape of our last layer (since both its components are), which perfectly matches its biases\n",
    "    gradient_b[-1] = delta\n",
    "    \n",
    "    # The weights in the last layer are matrix multiplied with the activations in the previous layer \n",
    "    # as part of computing the weighted input. \n",
    "    # We do the same here to find the gradient, but we must transpose it for the matrix multiply to \n",
    "    # have the right shape (that of the last layer weights)\n",
    "    # eg. If the last layer is 2 neurons, and the second to last is 5, then our weights are shape (2,5)\n",
    "    # delta is (2,1) and activations before are (5,1), transposed to (1,5)\n",
    "    gradient_w[-1] = np.matmul(delta, previous_activations.transpose())\n",
    "    \n",
    "    # Then we deal with each other layer, moving backward\n",
    "    # Using negative indices to count backward, so this is from 2nd-to-last layer to first layer\n",
    "    # When we index backward we count from 1 (ie last layer is -1), so first layer is at [-len(layers)]\n",
    "    # and range excludes the end point so we add 1 to include that first layer\n",
    "    for this_l in range(2, len(self.layers)+1):\n",
    "        this_layer = self.layers[-this_l]\n",
    "        this_weighted_input = weighted_inputs[-this_l]\n",
    "        layer_after_weights = self.layers[-this_l+1].weights # The weights of the layer after this one when going forward  \n",
    "        previous_activations = activations[-this_l-1] # We included input layer in activations\n",
    "        \n",
    "        previous_delta = delta # Same shape as the layer after this one\n",
    "        activation_derivative = this_layer.activation_function.derivative(this_weighted_input) # Same shape as this layer\n",
    "        \n",
    "        # We can't use the network cost function here, because we don't know exactly our target output,\n",
    "        # only the gradient we've computed for the layer after this one. We can find that cost though:\n",
    "        \n",
    "        # If we say that this layer has m neurons and the layer after this has m', then we know that \n",
    "        # the weights of the layer after this are shape (m', m)\n",
    "        # We know that the previous delta was shape (m', 1) - the shape of the layer after this\n",
    "        # Our cost is a function of these, and needs to be of the same shape as our activation function,\n",
    "        # which is (m, 1)\n",
    "        # So we matrix multiply the transpose of the weights (m, m') by the previous delta (m', 1), which gives\n",
    "        # us the correct shape.\n",
    "        cost_derivative = np.matmul(layer_after_weights.transpose(), previous_delta)\n",
    "        \n",
    "        delta = activation_derivative * cost_derivative\n",
    "        \n",
    "        # Again, delta is the shape of this layer since both its components are\n",
    "        gradient_b[-this_l] = delta\n",
    "        \n",
    "        # And again we matrix multiply it with the activations in the layer before\n",
    "        # to match the weights at this layer\n",
    "        gradient_w[-this_l] = np.dot(delta, previous_activations.transpose())\n",
    "\n",
    "    print activations[-1] # We'll comment this out if it gets annoying, but it's a useful illustration\n",
    "    print self.cost_function.compute(activations[-1], desired_output) # ^ Same with this\n",
    "    print\n",
    "    return (gradient_b, gradient_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.64194898]\n",
      " [ 0.53692561]]\n",
      "0.208244824507\n",
      "\n",
      "[[ 0.83275194]\n",
      " [ 0.18084706]]\n",
      "0.030338786921\n",
      "\n",
      "[[ 0.86954241]\n",
      " [ 0.13622218]]\n",
      "0.0177878319062\n",
      "\n",
      "[[ 0.88926067]\n",
      " [ 0.11409575]]\n",
      "0.0126405192036\n",
      "\n",
      "[[ 0.90212244]\n",
      " [ 0.10011386]]\n",
      "0.00980140031983\n",
      "\n",
      "[[ 0.91138025]\n",
      " [ 0.09022755]]\n",
      "0.00799723533138\n",
      "\n",
      "[[ 0.91845938]\n",
      " [ 0.08275459]]\n",
      "0.0067485974781\n",
      "\n",
      "[[ 0.92410049]\n",
      " [ 0.07684789]]\n",
      "0.00583316715057\n",
      "\n",
      "[[ 0.92873281]\n",
      " [ 0.07202678]]\n",
      "0.0051334346189\n",
      "\n",
      "[[ 0.93262492]\n",
      " [ 0.06799504]]\n",
      "0.00458136305167\n",
      "\n",
      "[[ 0.93595476]\n",
      " [ 0.06455867]]\n",
      "0.00413480731901\n",
      "\n",
      "[[ 0.93884555]\n",
      " [ 0.06158453]]\n",
      "0.00376626018708\n",
      "\n",
      "[[ 0.94138576]\n",
      " [ 0.05897777]]\n",
      "0.00345700317562\n",
      "\n",
      "[[ 0.94364074]\n",
      " [ 0.05666873]]\n",
      "0.00319385603922\n",
      "\n",
      "[[ 0.94565996]\n",
      " [ 0.05460495]]\n",
      "0.00296727001753\n",
      "\n",
      "[[ 0.94748169]\n",
      " [ 0.05274603]]\n",
      "0.00277015823704\n",
      "\n",
      "[[ 0.94913602]\n",
      " [ 0.05106032]]\n",
      "0.0025971508016\n",
      "\n",
      "[[ 0.95064699]\n",
      " [ 0.04952262]]\n",
      "0.00244410489127\n",
      "\n",
      "[[ 0.95203409]\n",
      " [ 0.04811256]]\n",
      "0.00230777350748\n",
      "\n",
      "[[ 0.95331328]\n",
      " [ 0.04681348]]\n",
      "0.00218557599492\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A very simple example, building up a network and training it to map a column [1,2,3,4] to a column [1,0]\n",
    "# This is an example of classification, in this case [1,2,3,4] is in the first class\n",
    "\n",
    "net1 = Network(QuadraticCost_cls())\n",
    "net1.add_layer(Layer(4, 5, Sigmoid_cls())) # Hidden layer\n",
    "net1.add_layer(Layer(5, 10, Sigmoid_cls())) # Hidden layer\n",
    "net1.add_layer(Layer(10, 30, Sigmoid_cls())) # Hidden layer\n",
    "net1.add_layer(Layer(30, 2, Sigmoid_cls())) # Output layer\n",
    "\n",
    "x = np.array([1,2,3,4]).reshape(4,1)\n",
    "y = np.array([1,0]).reshape(2,1)\n",
    "\n",
    "# This is basic gradient descent - backprop gives us the gradient for weights and biases, we descent in that direction\n",
    "for i in range(20):\n",
    "    g_b, g_w = net1.backprop(x,y)\n",
    "    for j in range(len(net1.layers)):\n",
    "        layer = net1.layers[j]\n",
    "        layer.weights -= g_w[j]\n",
    "        layer.bias -= g_b[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent\n",
    "\n",
    "If we have more than one training example, then we'll need a way to update the weights such that our cost function decreases by as much as possible over all of them - like with the perceptron. \n",
    "\n",
    "The batch gradient descent algorithm would involve computing the gradient (using backpropagation) for all training examples, and then updating it once - and then repeating that process for however many epochs we want to teach the network for. This uses a lot of memory and means we only descend the cost function after computing for all the training examples, and we can be liable to get stuck at a local minima. \n",
    "\n",
    "In stochastic gradient descent we instead descend the cost function after each individual training example. This is noisy but it still converges to the minima, and sometimes more noise is beneficial for escaping local minima. \n",
    "We actually used basically this for training our perceptron:\n",
    "```\n",
    "for x, y in training_data:\n",
    "    # Find error\n",
    "    w[0] += eta * error * x[0]\n",
    "    w[1] += eta * error * x[1]\n",
    "    bias += eta * error\n",
    "```\n",
    "\n",
    "Notice we update the weights and bias terms inside the loop over each training example. This is stochastic gradient descent. \n",
    "\n",
    "A compromise solution is the minibatch one, where we group our training data into batches on which we perform batch gradient descent sequentially. For example we might update the weights after every 64 training examples. This is much more efficient than stochastic gradient descent, but still has some noise and more regular descents than the pure batch method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a simple toy dataset of 500 examples, where each example has an input of shape (2,1) and each output has a shape (2,1)\n",
    "\n",
    "Our input values will all be between 0-1, and our target outputs will all be (1,0) or (0,1), ie another simple classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(shape=(2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
